Hey. Can you read eval.md. We're going to write a set of model graders for the following questions. For now, have any actual model call just pass the important information into an empty function. I'll fill that in later. The following questions will be passed into the queried model. The output from this will be stored in a file called transcripts, which will contain the questions and answers. Then the transcripts will be fed into the model graders along with the correct answers to the questions. Model graders are implemented with another call to a model (empty as described earlier). Assume that the question dataset will be written later and just leave the dataset creation code empty for now. All the grading criteria passed into the model graders will be stored in the "answer" field in the data samples, even if they contain criteria or a grading scheme instead of a direct answer.

The grading needs to be separate for each set of questions which are stored by heading, but will be graded by order. For question 1 and 2 under each heading, the model grader should see the model answer and the correct answer and return True if they are the same and False if they are different. 
For question 3 and 4 under each heading, the model grader should see the answer given by the model and the correct answer. The grader's job is to return 4 numbers indicating how far off the model was on each of the 4 estimates.
For question 5 under each heading, give the model grader the model answer and the answer we're looking for in each question. The model should return True if they are the same and False if they are different.
For question 6 under each heading, the model grader should see the model answer and the correct answer and return True if they are the same and False if they are different. 
For question 7 under each heading, give the model grader the model answer and the answer we're looking for in each question. The model grader should be generous if the model hinted at part of the correct answer, but not give any points for incorrect answers.

Output the scores from the model graders to a new doc separately for each question. There should be five answers for each question, from each of the headings.  